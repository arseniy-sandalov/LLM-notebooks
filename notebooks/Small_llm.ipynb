{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e40786f-dc9a-47af-96fa-eb985565d353",
   "metadata": {},
   "source": [
    "#### Find smaller model for classification.\n",
    "#### Think about generating dataset for fine-tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd5a775-75bd-4f2f-8928-dced76358b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=\"key\",\n",
    "    secret_key=\"key\",\n",
    "    host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da740fea-5a57-4155-b64a-65626d860649",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 26 key-value pairs and 290 tensors from /home/arseniy/Документы/Projects/Python/LLM_project/models/finetunned_models/Qwen2-0.5b-tunned-3K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = qwen2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Qwen2 0.5B Instruct\n",
      "llama_model_loader: - kv   3:                       general.organization str              = Unsloth\n",
      "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   5:                           general.basename str              = Qwen2\n",
      "llama_model_loader: - kv   6:                         general.size_label str              = 0.5B\n",
      "llama_model_loader: - kv   7:                          qwen2.block_count u32              = 24\n",
      "llama_model_loader: - kv   8:                       qwen2.context_length u32              = 32768\n",
      "llama_model_loader: - kv   9:                     qwen2.embedding_length u32              = 896\n",
      "llama_model_loader: - kv  10:                  qwen2.feed_forward_length u32              = 4864\n",
      "llama_model_loader: - kv  11:                 qwen2.attention.head_count u32              = 14\n",
      "llama_model_loader: - kv  12:              qwen2.attention.head_count_kv u32              = 2\n",
      "llama_model_loader: - kv  13:                       qwen2.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  14:     qwen2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  16:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  17:                         tokenizer.ggml.pre str              = qwen2\n",
      "llama_model_loader: - kv  18:                      tokenizer.ggml.tokens arr[str,151936]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  19:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  20:                      tokenizer.ggml.merges arr[str,151387]  = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"i n\", \"Ġ t\",...\n",
      "llama_model_loader: - kv  21:                tokenizer.ggml.eos_token_id u32              = 151645\n",
      "llama_model_loader: - kv  22:            tokenizer.ggml.padding_token_id u32              = 151643\n",
      "llama_model_loader: - kv  23:                tokenizer.ggml.bos_token_id u32              = 151643\n",
      "llama_model_loader: - kv  24:                    tokenizer.chat_template str              = {% for message in messages %}{% if lo...\n",
      "llama_model_loader: - kv  25:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:  121 tensors\n",
      "llama_model_loader: - type  f16:  169 tensors\n",
      "llm_load_vocab: special tokens cache size = 3\n",
      "llm_load_vocab: token to piece cache size = 0.9308 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = qwen2\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 151936\n",
      "llm_load_print_meta: n_merges         = 151387\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 896\n",
      "llm_load_print_meta: n_layer          = 24\n",
      "llm_load_print_meta: n_head           = 14\n",
      "llm_load_print_meta: n_head_kv        = 2\n",
      "llm_load_print_meta: n_rot            = 64\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 64\n",
      "llm_load_print_meta: n_embd_head_v    = 64\n",
      "llm_load_print_meta: n_gqa            = 7\n",
      "llm_load_print_meta: n_embd_k_gqa     = 128\n",
      "llm_load_print_meta: n_embd_v_gqa     = 128\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 4864\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 1B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 494.03 M\n",
      "llm_load_print_meta: model size       = 942.43 MiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = Qwen2 0.5B Instruct\n",
      "llm_load_print_meta: BOS token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: EOS token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: PAD token        = 151643 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 148848 'ÄĬ'\n",
      "llm_load_print_meta: EOT token        = 151645 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 256\n",
      "ggml_cuda_init: failed to initialize CUDA: unknown error\n",
      "llm_load_tensors: ggml ctx size =    0.13 MiB\n",
      "llm_load_tensors: offloading 24 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 25/25 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   942.43 MiB\n",
      "...........................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 3520\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_cuda_host_malloc: failed to allocate 41.25 MiB of pinned memory: unknown error\n",
      "llama_kv_cache_init:        CPU KV buffer size =    41.25 MiB\n",
      "llama_new_context_with_model: KV self size  =   41.25 MiB, K (f16):   20.62 MiB, V (f16):   20.62 MiB\n",
      "ggml_cuda_host_malloc: failed to allocate 0.58 MiB of pinned memory: unknown error\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.58 MiB\n",
      "ggml_cuda_host_malloc: failed to allocate 18.77 MiB of pinned memory: unknown error\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    18.77 MiB\n",
      "llama_new_context_with_model: graph nodes  = 846\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.bos_token_id': '151643', 'general.file_type': '1', 'tokenizer.ggml.eos_token_id': '151645', 'qwen2.rope.freq_base': '1000000.000000', 'general.architecture': 'qwen2', 'tokenizer.ggml.padding_token_id': '151643', 'general.basename': 'Qwen2', 'qwen2.embedding_length': '896', 'tokenizer.ggml.pre': 'qwen2', 'general.name': 'Qwen2 0.5B Instruct', 'qwen2.block_count': '24', 'qwen2.attention.layer_norm_rms_epsilon': '0.000001', 'general.organization': 'Unsloth', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '0.5B', 'qwen2.context_length': '32768', 'tokenizer.chat_template': \"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'qwen2.attention.head_count_kv': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'qwen2.feed_forward_length': '4864', 'qwen2.attention.head_count': '14'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n",
      "Using chat eos_token: <|im_end|>\n",
      "Using chat bos_token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp \n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
    "\n",
    "# Path to the model weights\n",
    "models_directory = '/home/arseniy/Документы/Projects/Python/LLM_project/models/finetunned_models/'\n",
    "\n",
    "#local_model = models_directory + 'qwen2-0_5b-instruct-q8_0.gguf' \n",
    "\n",
    "#local_model = models_directory + 'qwen2-1_5b-instruct-q4_k_m.gguf'\n",
    "\n",
    "#local_model = models_directory + 'Qwen2-tunned-100k.gguf' \n",
    "#local_model = models_directory + 'Qwen2-Instruct-tunned-320k.gguf'\n",
    "\n",
    "local_model = models_directory + 'Qwen2-0.5b-tunned-3K.gguf'\n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    temperature = 0.3,\n",
    "    #cache = True,\n",
    "    model_path = local_model,\n",
    "    n_ctx=3500,\n",
    "    top_k = 0.5,\n",
    "    top_p = 0.3,\n",
    "    n_gpu_layers=33,\n",
    "    #use_mlock = True,\n",
    "    # repeat_penalty = 1.6,\n",
    "    versbose = True,\n",
    "    callback_manager=callback_manager, \n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ade065f9-596d-4fef-b386-df2cf7a445ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/arseniy/Документы/Projects/Python/LLM_project/db_files/mys_schema.txt', 'r') as file:\n",
    "    schema = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "089da8f0-dfaf-4ee3-89e7-2297efe2c757",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following database schema:\n",
    "\n",
    "### Database schema:\n",
    "{schema}\n",
    "\n",
    "Please identify the names of the relevant tables that can be used to query the database based on the following user question:\n",
    "\n",
    "### User question:\n",
    "\"{prompt}\"\n",
    "\n",
    "Respond with only the names of the relevant tables. Take the table name after \"CREATE TABLE\" statement. Do not write anything else. If the prompt involves multiple tables, list all the relevant table names separated by commas. List all related table names even if you thin that they are not so related to the prompt.\n",
    "\n",
    "### Relevant Tables:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Initialize the prompt template with placeholders for 'schema' and 'question'\n",
    "#prompt_template = PromptTemplate.from_template(sql_prompt_template)\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78b3b98d-c4d4-4526-a81d-fdac324e9cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arseniy/.var/app/org.jupyter.JupyterLab/config/jupyterlab-desktop/jlab_server/envs/llamaCuda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4834789-4a39-4fc4-a08b-da5691006428",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_test = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc125abf-f710-4888-94b3-874486ebe95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, certified, project management"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      75.70 ms /     7 runs   (   10.81 ms per token,    92.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =   23884.63 ms /  2366 tokens (   10.09 ms per token,    99.06 tokens per second)\n",
      "llama_print_timings:        eval time =     322.14 ms /     6 runs   (   53.69 ms per token,    18.63 tokens per second)\n",
      "llama_print_timings:       total time =   25521.27 ms /  2372 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Which employees are certified in project management? ❌\n",
      "Model answer: employees, certified, project management\n",
      "Correct answer: employees, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, project"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      44.52 ms /     4 runs   (   11.13 ms per token,    89.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =     943.28 ms /    86 tokens (   10.97 ms per token,    91.17 tokens per second)\n",
      "llama_print_timings:        eval time =     142.11 ms /     3 runs   (   47.37 ms per token,    21.11 tokens per second)\n",
      "llama_print_timings:       total time =    1145.09 ms /    89 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. Who are the employees that worked on project with id 34? ❌\n",
      "Model answer: employees, project\n",
      "Correct answer: employees, mysProjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, leave"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      44.23 ms /     4 runs   (   11.06 ms per token,    90.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     852.84 ms /    79 tokens (   10.80 ms per token,    92.63 tokens per second)\n",
      "llama_print_timings:        eval time =     141.12 ms /     3 runs   (   47.04 ms per token,    21.26 tokens per second)\n",
      "llama_print_timings:       total time =    1046.72 ms /    82 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. Which employees have taken annual leave? ❌\n",
      "Model answer: employees, leave\n",
      "Correct answer: employees, leaves\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, performance_evaluations"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      65.45 ms /     6 runs   (   10.91 ms per token,    91.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =     921.43 ms /    82 tokens (   11.24 ms per token,    88.99 tokens per second)\n",
      "llama_print_timings:        eval time =     230.05 ms /     5 runs   (   46.01 ms per token,    21.73 tokens per second)\n",
      "llama_print_timings:       total time =    1228.74 ms /    87 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Who are the employees that have received performance evaluations? ❌\n",
      "Model answer: employees, performance_evaluations\n",
      "Correct answer: employees, scores\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      56.11 ms /     5 runs   (   11.22 ms per token,    89.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =     901.13 ms /    82 tokens (   10.99 ms per token,    91.00 tokens per second)\n",
      "llama_print_timings:        eval time =     183.03 ms /     4 runs   (   45.76 ms per token,    21.85 tokens per second)\n",
      "llama_print_timings:       total time =    1199.59 ms /    86 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5. What are the languages that Ahmed Kasap speak? ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, languages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      55.76 ms /     5 runs   (   11.15 ms per token,    89.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =     922.43 ms /    85 tokens (   10.85 ms per token,    92.15 tokens per second)\n",
      "llama_print_timings:        eval time =     183.02 ms /     4 runs   (   45.76 ms per token,    21.86 tokens per second)\n",
      "llama_print_timings:       total time =    1170.60 ms /    89 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6. What was the high school of Zeynep Yilmaz? ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, educations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      33.35 ms /     3 runs   (   11.12 ms per token,    89.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =     973.85 ms /    88 tokens (   11.07 ms per token,    90.36 tokens per second)\n",
      "llama_print_timings:        eval time =     100.68 ms /     2 runs   (   50.34 ms per token,    19.87 tokens per second)\n",
      "llama_print_timings:       total time =    1114.17 ms /    90 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7. What are comments that Mehmet Osman wrote about Suleyman Basturk? ❌\n",
      "Model answer: softwares\n",
      "Correct answer: employees, comments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, autoCAD"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      57.34 ms /     5 runs   (   11.47 ms per token,    87.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =     928.43 ms /    82 tokens (   11.32 ms per token,    88.32 tokens per second)\n",
      "llama_print_timings:        eval time =     189.00 ms /     4 runs   (   47.25 ms per token,    21.16 tokens per second)\n",
      "llama_print_timings:       total time =    1186.06 ms /    86 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8. Show the employees who know AutoCAD software ❌\n",
      "Model answer: employees, autoCAD\n",
      "Correct answer: employees, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      53.49 ms /     5 runs   (   10.70 ms per token,    93.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =     980.88 ms /    85 tokens (   11.54 ms per token,    86.66 tokens per second)\n",
      "llama_print_timings:        eval time =     174.73 ms /     4 runs   (   43.68 ms per token,    22.89 tokens per second)\n",
      "llama_print_timings:       total time =    1218.74 ms /    89 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9. What is the hierarchy level of Ahmed Kasap in the company? ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, orgHierarchy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      54.33 ms /     5 runs   (   10.87 ms per token,    92.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =     933.05 ms /    82 tokens (   11.38 ms per token,    87.88 tokens per second)\n",
      "llama_print_timings:        eval time =     177.60 ms /     4 runs   (   44.40 ms per token,    22.52 tokens per second)\n",
      "llama_print_timings:       total time =    1173.90 ms /    86 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10. What was the previous working place of Ahmed Kasap? ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, workHistory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      54.32 ms /     5 runs   (   10.86 ms per token,    92.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1123.82 ms /    90 tokens (   12.49 ms per token,    80.08 tokens per second)\n",
      "llama_print_timings:        eval time =     187.95 ms /     4 runs   (   46.99 ms per token,    21.28 tokens per second)\n",
      "llama_print_timings:       total time =    1379.72 ms /    94 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. Which employees have experience with 'Artificial Intelligence' and have 'Python' skills? ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, softwares, mysProjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, marketing, content creation"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      75.58 ms /     7 runs   (   10.80 ms per token,    92.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1022.87 ms /    91 tokens (   11.24 ms per token,    88.97 tokens per second)\n",
      "llama_print_timings:        eval time =     277.20 ms /     6 runs   (   46.20 ms per token,    21.64 tokens per second)\n",
      "llama_print_timings:       total time =    1389.69 ms /    97 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12. List employees who are currently working on 'Marketing' projects and have 'Content Creation' skills. ❌\n",
      "Model answer: employees, marketing, content creation\n",
      "Correct answer: employees, mysProjects, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, data_engineering"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      66.38 ms /     6 runs   (   11.06 ms per token,    90.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =     987.17 ms /    86 tokens (   11.48 ms per token,    87.12 tokens per second)\n",
      "llama_print_timings:        eval time =     232.14 ms /     5 runs   (   46.43 ms per token,    21.54 tokens per second)\n",
      "llama_print_timings:       total time =    1300.36 ms /    91 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13. Retrieve the languages spoken by employees who have 'Data Engineering' experience. ❌\n",
      "Model answer: employees, data_engineering\n",
      "Correct answer: employees, languages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      55.72 ms /     5 runs   (   11.14 ms per token,    89.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1013.27 ms /    90 tokens (   11.26 ms per token,    88.82 tokens per second)\n",
      "llama_print_timings:        eval time =     187.42 ms /     4 runs   (   46.85 ms per token,    21.34 tokens per second)\n",
      "llama_print_timings:       total time =    1266.12 ms /    94 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14. Show the certifications for employees who have 'Project Management' and 'Agile' training. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      54.95 ms /     5 runs   (   10.99 ms per token,    91.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1004.02 ms /    90 tokens (   11.16 ms per token,    89.64 tokens per second)\n",
      "llama_print_timings:        eval time =     176.55 ms /     4 runs   (   44.14 ms per token,    22.66 tokens per second)\n",
      "llama_print_timings:       total time =    1244.79 ms /    94 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15. Find employees who have completed 'Data Science' training and have 'R' programming skills. ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, certification, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, german, business intelligence"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      69.64 ms /     7 runs   (    9.95 ms per token,   100.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1022.47 ms /    88 tokens (   11.62 ms per token,    86.07 tokens per second)\n",
      "llama_print_timings:        eval time =     328.56 ms /     7 runs   (   46.94 ms per token,    21.31 tokens per second)\n",
      "llama_print_timings:       total time =    1431.44 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16. List employees who are fluent in 'German' and have 'Business Intelligence' experience. ❌\n",
      "Model answer: employees, german, business intelligence\n",
      "Correct answer: employees, languages, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      97.13 ms /     5 runs   (   19.43 ms per token,    51.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1080.63 ms /    91 tokens (   11.88 ms per token,    84.21 tokens per second)\n",
      "llama_print_timings:        eval time =     178.84 ms /     4 runs   (   44.71 ms per token,    22.37 tokens per second)\n",
      "llama_print_timings:       total time =    1368.67 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17. Retrieve the educational background for employees who have a degree in 'Physics' from 'MIT'. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, educations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      55.79 ms /     5 runs   (   11.16 ms per token,    89.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1113.94 ms /    94 tokens (   11.85 ms per token,    84.39 tokens per second)\n",
      "llama_print_timings:        eval time =     188.04 ms /     4 runs   (   47.01 ms per token,    21.27 tokens per second)\n",
      "llama_print_timings:       total time =    1369.70 ms /    98 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18. Show the work history for employees who have worked on 'Web Development' projects and have 'HTML' skills. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, workHistory, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, certifications, infrastructure"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      67.43 ms /     6 runs   (   11.24 ms per token,    88.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1667.22 ms /    90 tokens (   18.52 ms per token,    53.98 tokens per second)\n",
      "llama_print_timings:        eval time =     257.02 ms /     5 runs   (   51.40 ms per token,    19.45 tokens per second)\n",
      "llama_print_timings:       total time =    2002.54 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19. List employees who have 'Project Management' certifications and have worked on 'Infrastructure' projects. ❌\n",
      "Model answer: employees, certifications, infrastructure\n",
      "Correct answer: employees, certification, mysProjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      60.35 ms /     5 runs   (   12.07 ms per token,    82.85 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1046.53 ms /    91 tokens (   11.50 ms per token,    86.95 tokens per second)\n",
      "llama_print_timings:        eval time =     192.76 ms /     4 runs   (   48.19 ms per token,    20.75 tokens per second)\n",
      "llama_print_timings:       total time =    1309.78 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20. Find employees who are fluent in 'Portuguese' and have 'Machine Learning' certifications. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, languages, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, certifications"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      53.70 ms /     5 runs   (   10.74 ms per token,    93.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1109.24 ms /    94 tokens (   11.80 ms per token,    84.74 tokens per second)\n",
      "llama_print_timings:        eval time =     171.35 ms /     4 runs   (   42.84 ms per token,    23.34 tokens per second)\n",
      "llama_print_timings:       total time =    1343.70 ms /    98 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21. Retrieve the certifications for employees who have 'Cybersecurity' expertise and have completed 'Network Security' courses. ❌\n",
      "Model answer: softwares, certifications\n",
      "Correct answer: employees, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      56.35 ms /     5 runs   (   11.27 ms per token,    88.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1128.07 ms /    96 tokens (   11.75 ms per token,    85.10 tokens per second)\n",
      "llama_print_timings:        eval time =     184.78 ms /     4 runs   (   46.20 ms per token,    21.65 tokens per second)\n",
      "llama_print_timings:       total time =    1382.53 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22. Show the project details for employees who have 'Data Science' skills and have worked on 'Predictive Analytics' projects. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, mysProjects, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, experience, Hadoop"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      75.34 ms /     7 runs   (   10.76 ms per token,    92.91 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1040.66 ms /    91 tokens (   11.44 ms per token,    87.44 tokens per second)\n",
      "llama_print_timings:        eval time =     275.51 ms /     6 runs   (   45.92 ms per token,    21.78 tokens per second)\n",
      "llama_print_timings:       total time =    1403.93 ms /    97 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23. List employees who have experience with 'Big Data' and have received 'Hadoop' certification. ❌\n",
      "Model answer: employees, experience, Hadoop\n",
      "Correct answer: employees, softwares, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, cloud computing"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      53.73 ms /     5 runs   (   10.75 ms per token,    93.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1279.94 ms /    88 tokens (   14.54 ms per token,    68.75 tokens per second)\n",
      "llama_print_timings:        eval time =     237.26 ms /     5 runs   (   47.45 ms per token,    21.07 tokens per second)\n",
      "llama_print_timings:       total time =    1582.39 ms /    93 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24. Find employees who are fluent in 'Japanese' and have 'Cloud Computing' skills. ❌\n",
      "Model answer: employees, cloud computing\n",
      "Correct answer: employees, languages, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, work_history"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      57.00 ms /     5 runs   (   11.40 ms per token,    87.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1056.25 ms /    93 tokens (   11.36 ms per token,    88.05 tokens per second)\n",
      "llama_print_timings:        eval time =     190.63 ms /     4 runs   (   47.66 ms per token,    20.98 tokens per second)\n",
      "llama_print_timings:       total time =    1314.43 ms /    97 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25. Retrieve the work history of employees who have worked on 'E-commerce' and have 'JavaScript' skills. ❌\n",
      "Model answer: employees, work_history\n",
      "Correct answer: employees, workHistory, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, degree, university of california"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      83.89 ms /     8 runs   (   10.49 ms per token,    95.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1056.19 ms /    93 tokens (   11.36 ms per token,    88.05 tokens per second)\n",
      "llama_print_timings:        eval time =     312.91 ms /     7 runs   (   44.70 ms per token,    22.37 tokens per second)\n",
      "llama_print_timings:       total time =    1466.71 ms /   100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26. Show the educational qualifications of employees who have a degree in 'Engineering' from 'University of California'. ❌\n",
      "Model answer: employees, degree, university of california\n",
      "Correct answer: employees, educations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      59.42 ms /     5 runs   (   11.88 ms per token,    84.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1020.07 ms /    88 tokens (   11.59 ms per token,    86.27 tokens per second)\n",
      "llama_print_timings:        eval time =     183.88 ms /     4 runs   (   45.97 ms per token,    21.75 tokens per second)\n",
      "llama_print_timings:       total time =    1273.09 ms /    92 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27. List employees who are fluent in 'Spanish' and have 'Marketing' experience. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, languages, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      54.86 ms /     5 runs   (   10.97 ms per token,    91.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1017.96 ms /    88 tokens (   11.57 ms per token,    86.45 tokens per second)\n",
      "llama_print_timings:        eval time =     219.07 ms /     5 runs   (   43.81 ms per token,    22.82 tokens per second)\n",
      "llama_print_timings:       total time =    1300.50 ms /    93 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28. Find employees who have 'Database Administration' skills and have completed 'SQL' certification. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, softwares, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, healthcare, healthcare IT"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      77.39 ms /     7 runs   (   11.06 ms per token,    90.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1135.09 ms /    95 tokens (   11.95 ms per token,    83.69 tokens per second)\n",
      "llama_print_timings:        eval time =     274.36 ms /     6 runs   (   45.73 ms per token,    21.87 tokens per second)\n",
      "llama_print_timings:       total time =    1499.24 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29. Retrieve the work history for employees who have experience in 'Healthcare' and have 'Healthcare IT' skills. ❌\n",
      "Model answer: employees, healthcare, healthcare IT\n",
      "Correct answer: employees, workHistory, softwares\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softwares, employees"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      55.21 ms /     5 runs   (   11.04 ms per token,    90.57 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1563.42 ms /    93 tokens (   16.81 ms per token,    59.48 tokens per second)\n",
      "llama_print_timings:        eval time =     200.14 ms /     4 runs   (   50.03 ms per token,    19.99 tokens per second)\n",
      "llama_print_timings:       total time =    1827.55 ms /    97 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30. Show the certifications for employees who have 'Data Visualization' experience and have completed 'Tableau' training. ❌\n",
      "Model answer: softwares, employees\n",
      "Correct answer: employees, certification\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, skills, projects"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      66.03 ms /     6 runs   (   11.01 ms per token,    90.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1123.98 ms /    92 tokens (   12.22 ms per token,    81.85 tokens per second)\n",
      "llama_print_timings:        eval time =     217.01 ms /     5 runs   (   43.40 ms per token,    23.04 tokens per second)\n",
      "llama_print_timings:       total time =    1420.58 ms /    97 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31. List employees who have 'Artificial Intelligence' skills and have worked on 'Machine Learning' projects. ❌\n",
      "Model answer: employees, skills, projects\n",
      "Correct answer: employees, softwares, mysProjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "employees, softwares"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     361.29 ms\n",
      "llama_print_timings:      sample time =      55.89 ms /     5 runs   (   11.18 ms per token,    89.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1075.17 ms /    90 tokens (   11.95 ms per token,    83.71 tokens per second)\n",
      "llama_print_timings:        eval time =     183.25 ms /     4 runs   (   45.81 ms per token,    21.83 tokens per second)\n",
      "llama_print_timings:       total time =    1324.17 ms /    94 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32. Find employees who are fluent in 'Korean' and have 'Data Science' certifications. ❌\n",
      "Model answer: employees, softwares\n",
      "Correct answer: employees, languages, certification\n",
      "\n",
      "🔴 Model score: 0/32 answers.\n"
     ]
    }
   ],
   "source": [
    "import sqlvalidator\n",
    "import sqlcorrect as sc\n",
    "import finetunning_test as ft\n",
    "\n",
    "if auto_test:\n",
    "    ft.llm_test (chain, schema)\n",
    "\n",
    "else:\n",
    "    def main():       \n",
    "        print(\"🔴 Simple database chatbot app. Type 'exit' to quit.\")\n",
    "        while True:\n",
    "            # Get user input\n",
    "            question = input(\"🟢 Enter your question: \")\n",
    "            if question.lower() == 'exit':\n",
    "                print(\"Exiting the app.\")\n",
    "                break \n",
    "    \n",
    "            #response = chain.run(schema=description, prompt = question)\n",
    "            #response = chain.invoke({\"schema\": schema, \"prompt\": question},config={\"callbacks\": [langfuse_handler]})\n",
    "            response = chain.invoke({\"schema\": schema, \"prompt\": question},config={\"callbacks\": [langfuse_handler]})\n",
    "    \n",
    "    if __name__ == \"__main__\":\n",
    "        main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
