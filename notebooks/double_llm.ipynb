{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a4dd36d-2128-405e-9a43-2bbcb1356da7",
   "metadata": {},
   "source": [
    "#### Change the promting according to the promting of pip-sql-1.3b\n",
    "https://huggingface.co/PipableAI/pip-sql-1.3b\n",
    "\n",
    "#### Go from using langchain sql chain to manual promt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b66e310-16fa-4ef9-a9d9-4b122eee69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text, MetaData\n",
    "from langchain.sql_database import SQLDatabase\n",
    "\n",
    "db_files = '/home/arseniy/–î–æ–∫—É–º–µ–Ω—Ç—ã/Projects/Python/LLM_project/db_files/'\n",
    "\n",
    "engine = create_engine ('sqlite:///' + db_files + 'mys_db', echo= False, future=True)\n",
    "\n",
    "metadata = MetaData()\n",
    "metadata.reflect(bind=engine)\n",
    "\n",
    "db = SQLDatabase(engine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "895a96d1-aba9-4853-b62c-39326928b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langfuse.callback import CallbackHandler\n",
    "langfuse_handler = CallbackHandler(\n",
    "    public_key=\"key\",\n",
    "    secret_key=\"key\",\n",
    "    host=\"https://cloud.langfuse.com\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9750e48-998d-4a39-99a2-d103c94b16c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 29 key-value pairs and 291 tensors from /home/arseniy/–î–æ–∫—É–º–µ–Ω—Ç—ã/Projects/Python/LLM_project/models/Mistral-7B-Instruct-v0.3.Q5_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models--mistralai--Mistral-7B-Instruc...\n",
      "llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  11:                           llama.vocab_size u32              = 32768\n",
      "llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,32768]   = [\"<unk>\", \"<s>\", \"</s>\", \"[INST]\", \"[...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.scores arr[f32,32768]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  17:                  tokenizer.ggml.token_type arr[i32,32768]   = [2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  20:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  22:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  23:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  24:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  25:                      quantize.imatrix.file str              = ./imatrix.dat\n",
      "llama_model_loader: - kv  26:                   quantize.imatrix.dataset str              = group_40.txt\n",
      "llama_model_loader: - kv  27:             quantize.imatrix.entries_count i32              = 224\n",
      "llama_model_loader: - kv  28:              quantize.imatrix.chunks_count i32              = 74\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 771\n",
      "llm_load_vocab: token to piece cache size = 0.1731 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32768\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.25 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = models--mistralai--Mistral-7B-Instruct-v0.3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 781 '<0x0A>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "ggml_cuda_init: failed to initialize CUDA: unknown error\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors: offloading 10 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 10/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  4897.52 MiB\n",
      "warning: failed to mlock 93036544-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MEMLOCK ('ulimit -l' as root).\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 2016\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 32\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_cuda_host_malloc: failed to allocate 252.00 MiB of pinned memory: unknown error\n",
      "llama_kv_cache_init:        CPU KV buffer size =   252.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  252.00 MiB, K (f16):  126.00 MiB, V (f16):  126.00 MiB\n",
      "ggml_cuda_host_malloc: failed to allocate 0.12 MiB of pinned memory: unknown error\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.13 MiB\n",
      "ggml_cuda_host_malloc: failed to allocate 10.12 MiB of pinned memory: unknown error\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    10.12 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': 'group_40.txt', 'quantize.imatrix.chunks_count': '74', 'quantize.imatrix.file': './imatrix.dat', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.pre': 'default', 'llama.context_length': '32768', 'general.name': 'models--mistralai--Mistral-7B-Instruct-v0.3', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '17', 'llama.vocab_size': '32768', 'llama.rope.dimension_count': '128'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ntable_name_extractor = ChatLlamaCpp(\\n    cache = True,\\n    model_path = local_model,\\n    n_ctx=2000,\\n    n_gpu_layers=10,\\n    use_mlock = True,\\n    versbose = True,\\n    \\n)\\n\\nquery_generator = ChatLlamaCpp(\\n    cache = True,\\n    model_path = local_model,\\n    n_ctx=2000,\\n    n_gpu_layers=10,\\n    use_mlock = True,\\n    versbose = True,\\n    \\n)\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatLlamaCpp \n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain_core.callbacks import CallbackManager, StreamingStdOutCallbackHandler\n",
    "import multiprocessing\n",
    "from langchain.globals import set_llm_cache\n",
    "# Path to the model weights\n",
    "models_directory = '/home/arseniy/–î–æ–∫—É–º–µ–Ω—Ç—ã/Projects/Python/LLM_project/models/'\n",
    "'''\n",
    "small_model models_directory + 'qwen2-0_5b-instruct-q8_0.gguf'\n",
    "sql_model = models_directory + 'sqlcoder-7b-2.Q4_K_M.gguf' \n",
    "'''\n",
    "# First try using same big LLM in twice and keep it in memory.\n",
    "local_model = models_directory + 'Mistral-7B-Instruct-v0.3.Q5_K_M.gguf' \n",
    "\n",
    "llm = ChatLlamaCpp(\n",
    "    model_path = local_model,\n",
    "    n_ctx=2000,\n",
    "    n_gpu_layers=10,\n",
    "    use_mlock = True,\n",
    "    versbose = True,\n",
    ")\n",
    "\n",
    "'''\n",
    "table_name_extractor = ChatLlamaCpp(\n",
    "    cache = True,\n",
    "    model_path = local_model,\n",
    "    n_ctx=2000,\n",
    "    n_gpu_layers=10,\n",
    "    use_mlock = True,\n",
    "    versbose = True,\n",
    "    \n",
    ")\n",
    "\n",
    "query_generator = ChatLlamaCpp(\n",
    "    cache = True,\n",
    "    model_path = local_model,\n",
    "    n_ctx=2000,\n",
    "    n_gpu_layers=10,\n",
    "    use_mlock = True,\n",
    "    versbose = True,\n",
    "    \n",
    ")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "704695ff-cb32-4574-85b0-8ce8e0d2243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_community.tools.sql_database.tool import QuerySQLDataBaseTool\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a97af70e-7394-4bcb-8cfa-ebfad8fc5a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(db_files + 'raw.txt', 'r') as file:\n",
    "    schema = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "531b7a95-0898-49c9-a97a-75b0c06ca7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\n",
    "\n",
    "template = \"\"\"\n",
    "Given the following database schema:\n",
    "\n",
    "{schema}\n",
    "\n",
    "Please identify the names of the relevant tables that can be used to query the database based on the following user question:\n",
    "\n",
    "\"{prompt}\"\n",
    "\n",
    "Respond with only the names of the relevant tables.Do not write anything else. If the prompt involves multiple tables, list all the relevant table names separated by commas. List all related table names even if you thin that they are not so related to the prompt.\n",
    "\n",
    "\"\"\"\n",
    "# Initialize the prompt template with placeholders for 'schema' and 'question'\n",
    "prompt_template = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0912ff9f-69d1-4879-bd78-20e9a8f90169",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arseniy/.var/app/org.jupyter.JupyterLab/config/jupyterlab-desktop/jlab_server/envs/llamaCuda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "chain = LLMChain(prompt=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3973a85-512d-4c1c-8c89-40f7971f28fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "üü¢ Enter your question:  where is employee with id 3 living\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arseniy/.var/app/org.jupyter.JupyterLab/config/jupyterlab-desktop/jlab_server/envs/llamaCuda/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =     831.56 ms\n",
      "llama_print_timings:      sample time =       4.68 ms /    10 runs   (    0.47 ms per token,  2135.84 tokens per second)\n",
      "llama_print_timings: prompt eval time =  189559.88 ms /  1644 tokens (  115.30 ms per token,     8.67 tokens per second)\n",
      "llama_print_timings:        eval time =    1983.16 ms /     9 runs   (  220.35 ms per token,     4.54 tokens per second)\n",
      "llama_print_timings:       total time =  192243.29 ms /  1653 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " employees, orgHierarchy, workHistory\n"
     ]
    }
   ],
   "source": [
    "question = input(\"üü¢ Enter your question: \")\n",
    "response = chain.run(prompt = question, schema=schema)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2935ff-9822-4b6d-80ba-e0843186130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['employees', 'orgHierarchy', 'workHistory']\n"
     ]
    }
   ],
   "source": [
    "def parse_words(input_string):\n",
    "    # Remove leading and trailing spaces, then split by commas\n",
    "    words = input_string.strip().split(',')\n",
    "    \n",
    "    # Strip any leading/trailing spaces from each word\n",
    "    words = [word.strip() for word in words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "table_schemas = parse_words(response)\n",
    "print(table_schemas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c969a87-2842-44dc-a27a-602df6d4c211",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c826c95-0a5a-4d37-aa21-07478bbb6fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template1 =  \"\"\"\n",
    "                Generate a SQL query to answer the user question: \"{question}\".\n",
    "                \n",
    "                The query will run on a database with the following schema:\n",
    "                {table_schemas}\n",
    "                \n",
    "                Write only the query, nothing more!\n",
    "            \"\"\"\n",
    "prompt_template1 = PromptTemplate.from_template(template1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8b5c5cb8-4aac-4013-b860-8296690c6b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain1 = LLMChain(prompt=prompt_template1, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "20a30e77-12c5-4da7-8077-3251f6d9d39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     831.56 ms\n",
      "llama_print_timings:      sample time =      16.68 ms /    39 runs   (    0.43 ms per token,  2338.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     0 tokens (    -nan ms per token,     -nan tokens per second)\n",
      "llama_print_timings:        eval time =    7869.08 ms /    39 runs   (  201.77 ms per token,     4.96 tokens per second)\n",
      "llama_print_timings:       total time =    7904.10 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SELECT `address` FROM `employees` WHERE `id` = 3;\n",
      "\n",
      "This query will return the address of employee with id 3 from the employees table.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response1= chain1.run(question = question, table_schemas=table_schemas)\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37928f1-446f-4689-b0eb-80b3d9213e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import sqlvalidator\n",
    "import sqlcorrect as sc\n",
    "\n",
    "def main():\n",
    "    print(\"üî¥ Simple database chatbot app. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"üü¢ Enter your question: \")\n",
    "        if question.lower() == 'exit':\n",
    "            print(\"Exiting the app.\")\n",
    "            break \n",
    "\n",
    "        response = chain.run(question = question, schema=description)\n",
    "        try:            \n",
    "            print(\"Response:\", response) \n",
    "        except:\n",
    "            print(\"üî¥ LLM agent gave a wrong SQL query. Paraphrase the question...\")\n",
    "        \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2f18d0-40cb-4bed-9001-02f305cb4b37",
   "metadata": {},
   "source": [
    "#### Classify related table names with smaller model -> query the database with related names -> pass table schemas to the sqlcoder.\n",
    "#### Think about using cache, especially redis db as cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f3b3e-6222-4478-b2d1-48c0f3985d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
