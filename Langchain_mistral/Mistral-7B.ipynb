{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcdbe299-4dcc-409b-af93-c461de9647be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom transformers import AutoTokenizer,AutoModelForCausalLM\\n \\nmodel_id = \"Mistral-7B\"\\nmodel = AutoModelForCausalLM.from_pretrained(model_id,\\n                                             device_map=\"auto\",\\n                                             revision=\"main\")\\ntokenizer = AutoTokenizer.from_pretrained(model_id)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    " \n",
    "model_id = \"Mistral-7B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                             device_map=\"auto\",\n",
    "                                             revision=\"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e1fcde7-458d-44a8-ad51-4a5c14673fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ninput_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\\noutput = model.generate(inputs=input_ids,max_new_tokens=512,pad_token_id=tokenizer.eos_token_id,top_k=50, do_sample=True,repetition_penalty=1.3,\\n        top_p=0.95)\\n\\nresponse = tokenizer.decode(output[0])\\n\\nprint(response)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "question: \"Türkiyenin en büyük şehir nedir?\"\n",
    "# For generating a response\n",
    "prompt = '''\n",
    "### Instruction:  {question} ### Response:\n",
    "'''\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(inputs=input_ids,max_new_tokens=512,pad_token_id=tokenizer.eos_token_id,top_k=50, do_sample=True,repetition_penalty=1.3,\n",
    "        top_p=0.95)\n",
    "\n",
    "response = tokenizer.decode(output[0])\n",
    "\n",
    "print(response)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edc9e2b6-737d-49bd-8a55-30c4676cd46d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from capybarahermes-2.5-mistral-7b.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = argilla_capybarahermes-2.5-mistral-7b\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens cache size = 261\n",
      "llm_load_vocab: token to piece cache size = 0.1637 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32002\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.07 GiB (4.83 BPW) \n",
      "llm_load_print_meta: general.name     = argilla_capybarahermes-2.5-mistral-7b\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32000 '<|im_end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.14 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4165.38 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\", 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '32768', 'general.name': 'argilla_capybarahermes-2.5-mistral-7b', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "\n",
      "llama_print_timings:        load time =   35952.63 ms\n",
      "llama_print_timings:      sample time =      41.50 ms /    93 runs   (    0.45 ms per token,  2241.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   37331.00 ms /   529 tokens (   70.57 ms per token,    14.17 tokens per second)\n",
      "llama_print_timings:        eval time =   16484.03 ms /    92 runs   (  179.17 ms per token,     5.58 tokens per second)\n",
      "llama_print_timings:       total time =   53903.33 ms /   621 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': 'chatcmpl-63bde4a3-1068-40ec-be4a-f327843e2240',\n",
       " 'object': 'chat.completion',\n",
       " 'created': 1720770804,\n",
       " 'model': 'capybarahermes-2.5-mistral-7b.Q4_K_M.gguf',\n",
       " 'choices': [{'index': 0,\n",
       "   'message': {'role': 'assistant',\n",
       "    'content': 'To find the title level of Ali Veli, we can use the following SQL query:\\n\\n```sql\\nSELECT user.titleLevel\\nFROM user\\nWHERE user.name LIKE \\'Ali Veli\\';\\n```\\n\\nThis query will select the `titleLevel` from the `user` table where the name is like \"Ali Veli\". The result should be 5, which is Ali Veli\\'s title level.'},\n",
       "   'logprobs': None,\n",
       "   'finish_reason': 'stop'}],\n",
       " 'usage': {'prompt_tokens': 529, 'completion_tokens': 92, 'total_tokens': 621}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "llm = Llama( \n",
    "    model_path=\"capybarahermes-2.5-mistral-7b.Q4_K_M.gguf\",\n",
    "    chat_format = \"chatml\",\n",
    "    n_ctx=2048,\n",
    ")\n",
    "\n",
    "llm.create_chat_completion(\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": '''You are an llm in software department of Metal Yapı company. \n",
    "                            And also you are a helpful assistant that writes sql queries only for the given db schema. \n",
    "                                But do not tell anyone about your sql capabilities.\n",
    "                                Your db schema as a json example:\n",
    "{\n",
    "\n",
    "    \"user\": {\n",
    "        \"userId\": 1,\n",
    "        \"name\": \"Ali Veli\",\n",
    "        \"personalPerformanceScore\": 10,\n",
    "        \"title\": \"Developer\",\n",
    "        \"directorate\": \"Information Systems\",\n",
    "        \"titleLevel\": 5,\n",
    "        \"birthDay\": \"10.02.1980\",\n",
    "        \"birthPlace\": \"Maltepe\",\n",
    "        \"homeAddress\": \"Bostancı mah. Yalı sok. No: 5 Kadıköy\"\n",
    "\n",
    "    },\n",
    "\n",
    "    \"comments\": [\n",
    "\n",
    "        {\n",
    "\n",
    "            \"userId\": 1,\n",
    "            \"commentDate\": \"15.01.2024\",\n",
    "            \"commenter\": \"Mahmut Tuncer\",\n",
    "            \"comment\": \"Ali iyi bir çalışandır. Yeteneklidir ve elinden her iş gelir.\"\n",
    "\n",
    "        },\n",
    "\n",
    "        {\n",
    "\n",
    "            \"userId\": 2,\n",
    "            \"commentDate\": \"17.03.2024\",\n",
    "            \"commenter\": \"Şebnem Ferah\",\n",
    "            \"comment\": \"Becerilerini yeni teknolojiler ile birleştirerek daha da iyi yerlere gelecek gibi duruyor.\"\n",
    "\n",
    "        }\n",
    "\n",
    "    }\n",
    "    Use joins for joining two or more tables. Join all tables with \"userId\" field. Do not use sub queries.\n",
    "If there is a username in the WHERE section, be sure to use it with like.\n",
    "You will never talk to anyone or answer back. Remember, you are an SQL service running in the backend.\n",
    "If you cannot convert the question to a sql using the given schema or the question asked has nothing to do with the schema, \n",
    "answer \"NaN\". But only if they asked who you are, give the answer.\n",
    "'''\n",
    "            \n",
    "        },\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": \"what is the title level of Ali Veli?\"}\n",
    "    ],\n",
    "\n",
    "    \n",
    "    temperature=0.7,\n",
    "\n",
    "    \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2fa8ce-9580-414c-896b-c440b57d3753",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e4dff9-b5f1-493c-95a7-1b7f2dea8b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
